{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":5179545,"sourceType":"datasetVersion","datasetId":3011225},{"sourceId":10889949,"sourceType":"datasetVersion","datasetId":6767223}],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\nimport os\n\n# Setting up the enviorenment for pytorch memory allocation. To avoid un allocated memory\nos.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n\n\nimport kagglehub\nfrom sklearn.preprocessing import StandardScaler\nfrom glob import glob\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport matplotlib.image as mpimg\nfrom skimage.transform import resize\nimport pandas as pd\nfrom matplotlib.image import imread\nfrom skimage.io import imread_collection\nfrom PIL import Image\nimport seaborn as sns\nfrom sklearn import decomposition, preprocessing, svm\nimport sklearn.metrics as metrics #confusion_matrix, accuracy_score\nfrom time import sleep \nfrom tqdm.notebook import tqdm\nimport os\nsns.set()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-18T16:24:29.858294Z","iopub.execute_input":"2025-06-18T16:24:29.858714Z","iopub.status.idle":"2025-06-18T16:24:29.866241Z","shell.execute_reply.started":"2025-06-18T16:24:29.858670Z","shell.execute_reply":"2025-06-18T16:24:29.865307Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Download latest version\npath = kagglehub.dataset_download(\"lukechugh/best-alzheimer-mri-dataset-99-accuracy\")\n\nprint(\"Path to dataset files:\", path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T16:24:29.867197Z","iopub.execute_input":"2025-06-18T16:24:29.867458Z","iopub.status.idle":"2025-06-18T16:24:30.021379Z","shell.execute_reply.started":"2025-06-18T16:24:29.867426Z","shell.execute_reply":"2025-06-18T16:24:30.020650Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import glob\n\n# Dataset that should go with Alzheimer test label\nvery_mild_test = glob.glob(r'/kaggle/input/d/lukechugh/best-alzheimer-mri-dataset-99-accuracy/Combined Dataset/test/Very Mild Impairment/*')\nmild_test = glob.glob(r'/kaggle/input/d/lukechugh/best-alzheimer-mri-dataset-99-accuracy/Combined Dataset/test/Mild Impairment/*')\nmoderate_test = glob.glob(r'/kaggle/input/d/lukechugh/best-alzheimer-mri-dataset-99-accuracy/Combined Dataset/test/Moderate Impairment/*')\n\n# Dataset that should go with Alzheimer train label\nvery_mild_train = glob.glob(r'/kaggle/input/d/lukechugh/best-alzheimer-mri-dataset-99-accuracy/Combined Dataset/train/Very Mild Impairment/*')\nmild_train = glob.glob(r'/kaggle/input/d/lukechugh/best-alzheimer-mri-dataset-99-accuracy/Combined Dataset/train/Mild Impairment/*')\nmoderate_train = glob.glob(r'/kaggle/input/d/lukechugh/best-alzheimer-mri-dataset-99-accuracy/Combined Dataset/train/Moderate Impairment/*')\n\n\n# Dataset without Alzheimer for test\nnon_test = glob.glob(r'/kaggle/input/d/lukechugh/best-alzheimer-mri-dataset-99-accuracy/Combined Dataset/test/No Impairment/*')\n\n# Dataset without Alzheimer for train\nnon_train = glob.glob(r'/kaggle/input/d/lukechugh/best-alzheimer-mri-dataset-99-accuracy/Combined Dataset/train/No Impairment/*')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T16:24:30.023154Z","iopub.execute_input":"2025-06-18T16:24:30.023366Z","iopub.status.idle":"2025-06-18T16:24:30.056656Z","shell.execute_reply.started":"2025-06-18T16:24:30.023347Z","shell.execute_reply":"2025-06-18T16:24:30.055816Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Viewing the Images**","metadata":{}},{"cell_type":"code","source":"print(non_train[1])\ndef view_image(directory):\n    img = mpimg.imread(directory)\n    plt.imshow(img)\n    plt.title(directory)\n    plt.axis('off')\n    print(f'Image shape:{img.shape}')\n    return img\n\nprint('One of the data in Non Alzheimer Folder')\n\nview_image(moderate_train[1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T16:24:30.058197Z","iopub.execute_input":"2025-06-18T16:24:30.058439Z","iopub.status.idle":"2025-06-18T16:24:30.291498Z","shell.execute_reply.started":"2025-06-18T16:24:30.058418Z","shell.execute_reply":"2025-06-18T16:24:30.290688Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Combining the dataset**","metadata":{}},{"cell_type":"code","source":"#Imports\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import transforms\nfrom einops import rearrange, repeat\nfrom einops.layers.torch import Rearrange\n\n\n# Data preprocessing transforms\ndef get_transforms(image_size=224):\n    return transforms.Compose([\n        transforms.Resize((image_size, image_size)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize\n    ])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T16:24:30.292505Z","iopub.execute_input":"2025-06-18T16:24:30.292923Z","iopub.status.idle":"2025-06-18T16:24:30.298783Z","shell.execute_reply.started":"2025-06-18T16:24:30.292889Z","shell.execute_reply":"2025-06-18T16:24:30.297926Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\nfrom PIL import Image\n\nclass AlzheimerDataset(Dataset):\n    def __init__(self, image_paths, labels, transform=None):\n        \"\"\"\n        Args:\n            image_paths (list of str): List of file paths to the images.\n            labels (list of int): List of labels corresponding to each image.\n            transform (callable, optional): Transformations to be applied to the images.\n        \"\"\"\n        self.image_paths = image_paths\n        self.labels = labels\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        # Load the image\n        img_path = self.image_paths[idx]\n        image = Image.open(img_path).convert(\"RGB\")  # can turn it to rgb using .convert(\"RGB\")\n\n        # Apply transformations if provided\n        if self.transform:\n            image = self.transform(image)\n\n        # Get the corresponding label\n        label = self.labels[idx]\n\n        return image, label\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T16:24:30.299650Z","iopub.execute_input":"2025-06-18T16:24:30.299916Z","iopub.status.idle":"2025-06-18T16:24:30.316980Z","shell.execute_reply.started":"2025-06-18T16:24:30.299896Z","shell.execute_reply":"2025-06-18T16:24:30.316216Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Paths and labels\ntrain_paths = very_mild_train + mild_train + moderate_train + non_train\ntrain_labels = [1] * len(very_mild_train) + [2] * len(mild_train) + [3] * len(moderate_train) + [0] * len(non_train)\n\ntest_paths = very_mild_test + mild_test + moderate_test + non_test\ntest_labels = [1] * len(very_mild_test) + [2] * len(mild_test) + [3] * len(moderate_test) + [0] * len(non_test)\n\n# Apply transforms\ntransform = get_transforms(image_size=224)\n\n# Create datasets\ntrain_dataset = AlzheimerDataset(train_paths, train_labels, transform=transform)\ntest_dataset = AlzheimerDataset(test_paths, test_labels, transform=transform)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T16:24:30.317717Z","iopub.execute_input":"2025-06-18T16:24:30.317970Z","iopub.status.idle":"2025-06-18T16:24:30.331779Z","shell.execute_reply.started":"2025-06-18T16:24:30.317938Z","shell.execute_reply":"2025-06-18T16:24:30.331021Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# The RESNET model to train the data on","metadata":{}},{"cell_type":"markdown","source":"# 2. Initializing the RESNET model for the training","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import models\n\n# CBAM Attention Module\nclass ChannelAttention(nn.Module):\n    def __init__(self, in_planes, ratio=16):\n        super().__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.max_pool = nn.AdaptiveMaxPool2d(1)\n\n        self.fc = nn.Sequential(\n            nn.Conv2d(in_planes, in_planes // ratio, 1, bias=False),\n            nn.ReLU(),\n            nn.Conv2d(in_planes // ratio, in_planes, 1, bias=False)\n        )\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        avg_out = self.fc(self.avg_pool(x))\n        max_out = self.fc(self.max_pool(x))\n        out = avg_out + max_out\n        return self.sigmoid(out)\n\nclass SpatialAttention(nn.Module):\n    def __init__(self, kernel_size=7):\n        super().__init__()\n        self.conv = nn.Conv2d(2, 1, kernel_size, padding=kernel_size//2, bias=False)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        avg_out = torch.mean(x, dim=1, keepdim=True)\n        max_out, _ = torch.max(x, dim=1, keepdim=True)\n        x = torch.cat([avg_out, max_out], dim=1)\n        return self.sigmoid(self.conv(x))\n\nclass CBAM(nn.Module):\n    def __init__(self, in_planes, ratio=16, kernel_size=7):\n        super().__init__()\n        self.ca = ChannelAttention(in_planes, ratio)\n        self.sa = SpatialAttention(kernel_size)\n\n    def forward(self, x):\n        x = x * self.ca(x)\n        x = x * self.sa(x)\n        return x\n\n# Updated ResNet101 with CBAM\nclass ResNet101_CBAM_MultiClass(nn.Module):\n    def __init__(self, num_classes=4, pretrained=True, freeze_backbone=True):\n        super().__init__()\n        self.backbone = models.resnet50(pretrained=pretrained)\n\n        if freeze_backbone:\n            for param in self.backbone.parameters():\n                param.requires_grad = False\n\n        # Inject CBAM after layer4\n        self.cbam = CBAM(in_planes=2048)\n\n        # Replace the FC layer\n        self.backbone.fc = nn.Sequential(\n            nn.Linear(2048, 512),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(512, num_classes)\n        )\n\n    def forward(self, x):\n        x = self.backbone.conv1(x)\n        x = self.backbone.bn1(x)\n        x = self.backbone.relu(x)\n        x = self.backbone.maxpool(x)\n\n        x = self.backbone.layer1(x)\n        x = self.backbone.layer2(x)\n        x = self.backbone.layer3(x)\n        x = self.backbone.layer4(x)\n\n        x = self.cbam(x)  # Apply attention\n\n        x = self.backbone.avgpool(x)\n        x = torch.flatten(x, 1)\n        x = self.backbone.fc(x)\n        return x\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T16:42:51.941335Z","iopub.execute_input":"2025-06-18T16:42:51.941681Z","iopub.status.idle":"2025-06-18T16:42:51.954420Z","shell.execute_reply.started":"2025-06-18T16:42:51.941657Z","shell.execute_reply":"2025-06-18T16:42:51.953400Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 1. Setting up the data","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\n# Hyperparameters\nBATCH_SIZE = 32\nNUM_CLASSES = 4  # Non-demented, Very Mild, Mild, Moderate\nIMAGE_SIZE = 224\n\n# Create DataLoaders\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T16:42:51.955687Z","iopub.execute_input":"2025-06-18T16:42:51.956009Z","iopub.status.idle":"2025-06-18T16:42:51.971700Z","shell.execute_reply.started":"2025-06-18T16:42:51.955979Z","shell.execute_reply":"2025-06-18T16:42:51.970650Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# Device configuration\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# Initializing the model\nnum_classes = 4 \nmodel = ResNet101_CBAM_MultiClass(num_classes=4, pretrained=True, freeze_backbone=False).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)\n\n#using multiple gpu's in case they are available\nif torch.cuda.device_count() > 1:\n    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n    model = nn.DataParallel(model)\n\nmodel = model.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T16:42:51.973617Z","iopub.execute_input":"2025-06-18T16:42:51.973893Z","iopub.status.idle":"2025-06-18T16:42:53.030331Z","shell.execute_reply.started":"2025-06-18T16:42:51.973872Z","shell.execute_reply":"2025-06-18T16:42:53.029648Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 3. Defining the loss function and the optimizer","metadata":{}},{"cell_type":"code","source":"import torch.optim as optim\nfrom transformers import get_cosine_schedule_with_warmup\nEPOCHS = 10\n# Schedular for the warmup\nscheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=100, num_training_steps=len(train_loader)*EPOCHS)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T16:55:35.205639Z","iopub.execute_input":"2025-06-18T16:55:35.206022Z","iopub.status.idle":"2025-06-18T16:55:35.210580Z","shell.execute_reply.started":"2025-06-18T16:55:35.205993Z","shell.execute_reply":"2025-06-18T16:55:35.209665Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Training function for one epoch\ndef train_epoch(model, dataloader, criterion, optimizer, device):\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n\n    for inputs, labels in dataloader:\n        inputs, labels = inputs.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n        outputs = model(inputs)\n\n        loss = criterion(outputs, labels)\n        _, predicted = outputs.max(1)\n\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n    epoch_loss = running_loss / len(dataloader)\n    epoch_acc = 100 * correct / total\n    return epoch_loss, epoch_acc\n\n# Validation function\ndef validate(model, dataloader, criterion, device):\n    model.eval()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n\n    with torch.no_grad():\n        for inputs, labels in dataloader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n\n            loss = criterion(outputs, labels)\n            _, predicted = outputs.max(1)\n\n            running_loss += loss.item()\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n    val_loss = running_loss / len(dataloader)\n    val_acc = 100 * correct / total\n    return val_loss, val_acc\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T16:55:35.211924Z","iopub.execute_input":"2025-06-18T16:55:35.212237Z","iopub.status.idle":"2025-06-18T16:55:35.235263Z","shell.execute_reply.started":"2025-06-18T16:55:35.212184Z","shell.execute_reply":"2025-06-18T16:55:35.234493Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 4. Loop for the training with the model","metadata":{}},{"cell_type":"code","source":"def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, device, epochs=10):\n    for epoch in range(epochs):\n        print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n        \n        # Train the model for one epoch\n        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n        print(f\"Training - Loss: {train_loss:.4f}, Accuracy: {train_acc:.2f}%\")\n        \n        # Validate the model\n        val_loss, val_acc = validate(model, test_loader, criterion, device)\n        print(f\"Validation - Loss: {val_loss:.4f}, Accuracy: {val_acc:.2f}%\")\n        \n        # Step the scheduler based on validation loss\n        scheduler.step(val_loss)\n        \n        # Monitor the learning rate\n        for param_group in optimizer.param_groups:\n            print(f\"Learning Rate: {param_group['lr']}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T16:55:35.236328Z","iopub.execute_input":"2025-06-18T16:55:35.236548Z","iopub.status.idle":"2025-06-18T16:55:35.252156Z","shell.execute_reply.started":"2025-06-18T16:55:35.236529Z","shell.execute_reply":"2025-06-18T16:55:35.251359Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 5. Loop for the evaluation while training","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix, f1_score\nimport numpy as np\n\ndef evaluate_model(model, test_loader, criterion, device):\n    model.eval()  # Set the model to evaluation mode\n    all_preds = []\n    all_targets = []\n    test_loss = 0.0\n\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            \n            # Compute loss\n            if isinstance(criterion, nn.BCELoss):\n                target = target.float()\n                loss = criterion(output.squeeze(), target)\n                predicted = (output.squeeze() > 0.5).float()\n            else:\n                loss = criterion(output, target)\n                _, predicted = torch.max(output.data, 1)\n            \n            test_loss += loss.item()\n            \n            # Collect predictions and true labels\n            all_preds.extend(predicted.cpu().numpy())\n            all_targets.extend(target.cpu().numpy())\n\n    # Calculate average test loss\n    test_loss /= len(test_loader)\n\n    # Convert to NumPy arrays\n    all_preds = np.array(all_preds)\n    all_targets = np.array(all_targets)\n\n    # Calculate accuracy\n    accuracy = 100 * (all_preds == all_targets).sum() / len(all_targets)\n\n    # Generate classification report\n    class_report = classification_report(all_targets, all_preds, target_names=[\"Non-Demented\", \"Very Mild\", \"Mild\", \"Moderate\"])\n\n    # Generate confusion matrix\n    conf_matrix = confusion_matrix(all_targets, all_preds)\n\n    # Compute F1 score (macro-average)\n    f1 = f1_score(all_targets, all_preds, average='macro')\n\n    # Print results\n    print(f\"Test Loss: {test_loss:.4f}\")\n    print(f\"Test Accuracy: {accuracy:.2f}%\")\n    print(\"\\nClassification Report:\\n\", class_report)\n    print(\"\\nConfusion Matrix:\\n\", conf_matrix)\n    print(f\"\\nMacro F1 Score: {f1:.2f}\")\n\n    return accuracy, test_loss, f1, class_report, conf_matrix\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T16:55:35.253203Z","iopub.execute_input":"2025-06-18T16:55:35.253472Z","iopub.status.idle":"2025-06-18T16:55:35.271390Z","shell.execute_reply.started":"2025-06-18T16:55:35.253443Z","shell.execute_reply":"2025-06-18T16:55:35.270687Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Executing the training and evaluation functions","metadata":{}},{"cell_type":"code","source":"# Train the model\ntrain_model(model, train_loader, test_loader, criterion, optimizer, scheduler, device, epochs=10)\n\n# Evaluate the model\nevaluate_model(model, test_loader, criterion, device)\n\n# Save the trained model to a file in the Kaggle working directory\nmodel_save_path = \"/kaggle/working/alzheimers_model.pth\"\ntorch.save(model.state_dict(), model_save_path)\n\nprint(f\"Model saved to {model_save_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T16:55:35.272266Z","iopub.execute_input":"2025-06-18T16:55:35.272551Z","iopub.status.idle":"2025-06-18T17:05:56.030012Z","shell.execute_reply.started":"2025-06-18T16:55:35.272521Z","shell.execute_reply":"2025-06-18T17:05:56.029135Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Function to predict an image","metadata":{}},{"cell_type":"code","source":"from PIL import Image\nimport torch\nfrom torchvision import transforms\n\n\n# Function to preprocess and predict\ndef predict_image(model, image_path, device, class_names):\n    # Load the image\n    image = Image.open(image_path).convert('RGB')\n    \n    # Define transformations (same as used during training)\n    transform = transforms.Compose([\n        transforms.Resize((224, 224)),  # Resize to 224x224\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize\n    ])\n    \n    # Preprocess the image\n    image_tensor = transform(image).unsqueeze(0)  # Add batch dimension\n    \n    # Move to device\n    image_tensor = image_tensor.to(device)\n    \n    # Set the model to evaluation mode\n    model.eval()\n    with torch.no_grad():\n        # Get predictions\n        output = model(image_tensor)\n        probabilities = torch.softmax(output[0], dim=0)  # Applying softmax\n        predicted_class = probabilities.argmax().item()  # Geting the index of the predicted class\n    \n    # Print result\n    print(f\"Predicted category: {predicted_class}\")\n    print(f\"Probabilities: {probabilities.cpu().numpy()}\")\n    \n    return class_names[predicted_class]\n\n# Define class names (replace with your actual classes)\nclass_names = ['Non Demented', 'Very Mild Demented', 'Mild Demented', 'Moderately Demented']\n\n# Path to the image\nimage_path = non_test[2]\n\n# Predict category\npredicted_category = predict_image(model, image_path, device, class_names)\nprint(f\"The Alzheimer's disease category is: {predicted_category}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T17:05:56.031664Z","iopub.execute_input":"2025-06-18T17:05:56.031994Z","iopub.status.idle":"2025-06-18T17:05:56.052823Z","shell.execute_reply.started":"2025-06-18T17:05:56.031968Z","shell.execute_reply":"2025-06-18T17:05:56.052097Z"}},"outputs":[],"execution_count":null}]}